{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from algebra.cliffordalgebra import CliffordAlgebra\n",
    "from models.modules.linear import MVLinear\n",
    "from models.modules.gp import SteerableGeometricProductLayer\n",
    "from models.modules.mvlayernorm import MVLayerNorm\n",
    "from models.modules.mvsilu import MVSiLU\n",
    "from models.nbody_cggnn import CEMLP\n",
    "from data import nbody\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric for 3D space (Euclidean)\n",
    "metric = [1, 1, 1]\n",
    "d = len(metric)\n",
    "\n",
    "# Initialize the Clifford Algebra for 3D\n",
    "clifford_algebra = CliffordAlgebra(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_edge_attr(node_features, edges, batch_size):\n",
    "    edge_attributes = []\n",
    "\n",
    "    total_number_edges = edges[0].shape[0]\n",
    "\n",
    "    # Loop over all edges\n",
    "    for i in range(total_number_edges):\n",
    "\n",
    "        node1 = edges[0][i]\n",
    "        node2 = edges[1][i]\n",
    "\n",
    "        # difference between node features\n",
    "        node_i_features = node_features[node1]  # [#features(charge, loc, vel), dim]\n",
    "        node_j_features = node_features[node2]  # [#features(charge, loc, vel), dim]\n",
    "        difference = node_i_features - node_j_features\n",
    "        edge_representation = difference\n",
    "        edge_attributes.append(edge_representation)\n",
    "\n",
    "    edge_attributes = torch.stack(edge_attributes)\n",
    "    return edge_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_nbody_graphs(batch):\n",
    "    loc, vel, edge_attr, charges, loc_end, edges = batch\n",
    "\n",
    "    batch_size, n_nodes, _ = loc.size()\n",
    "    _, n_edges, _ = edge_attr.size()\n",
    "\n",
    "    # put the mean of the pointcloud on the origin -> translation invariant\n",
    "    loc_mean = loc - loc.mean(dim=1, keepdim=True)     # [batch, nodes, dim]\n",
    "\n",
    "    # All times do [batch, nodes, dim] -> [batch * nodes, dim]\n",
    "    loc_mean = loc_mean.float().view(-1, *loc_mean.shape[2:])\n",
    "    loc = loc.float().view(-1, *loc.shape[2:])\n",
    "    vel = vel.float().view(-1, *vel.shape[2:])\n",
    "    edge_attr = edge_attr.float().view(-1, *edge_attr.shape[2:])\n",
    "    charges = charges.float().view(-1, *charges.shape[2:])\n",
    "    loc_end = loc_end.float().view(-1, *loc_end.shape[2:])\n",
    "\n",
    "    invariants = charges\n",
    "    invariants = clifford_algebra.embed(invariants, (0,))\n",
    "\n",
    "    xv = torch.stack([loc_mean, vel], dim=1) # now of the shape [batch * nodes, 2 (because its loc mean as well as vel), dim]\n",
    "    covariants = clifford_algebra.embed(xv, (1, 2, 3))\n",
    "\n",
    "    # Create a vector with [n1, n2, n3, e1, e2, e3, e4]\n",
    "    nodes_in_clifford = torch.cat([invariants[:, None], covariants], dim=1) # [batch * nodes, #features(charge, loc, vel), dim]\n",
    "    zero_padding_nodes = torch.zeros(5*batch_size, 4, 8)\n",
    "    full_node_embedding = torch.cat((nodes_in_clifford, zero_padding_nodes), dim=1)\n",
    "\n",
    "\n",
    "    # IDK dont really know if this all is needed, just the start and end nodes should be enough but we will see\n",
    "    batch_index = torch.arange(batch_size, device=loc_mean.device) # torch.arange(batch_size) generates a tensor from 0 to batch_size - 1, creating a sequence that represents each graph in the batch. If batch_size is 3, this tensor will be [0, 1, 2]\n",
    "    edges = edges + n_nodes * batch_index[:, None, None] # creates separate edge number for every graph. so if edge for graph 1 is between 3 and 4, graph 2 will be between 8 and 9 (if n_nodes = 5)\n",
    "    edges = tuple(edges.transpose(0, 1).flatten(1)) # where the first element of the tuple contains all start nodes and the second contains all end nodes for edges across the entire batch. ([edges*batch], [edges*batch])\n",
    "    start_nodes, end_nodes = edges\n",
    "\n",
    "    # Initialize an attention mask with zeros (disallow all attention initially) NO IDEA IF THIS WORKS FOR MORE IN ONE BATCH\n",
    "    attention_mask = torch.zeros((n_nodes + n_edges)*batch_size, (n_nodes + n_edges)*batch_size)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        node_start_idx = b * (n_nodes + n_edges)\n",
    "        edge_start_idx = node_start_idx + 5\n",
    "\n",
    "        # Nodes can attend to themselves and to all other nodes within the same graph\n",
    "        for i in range(n_nodes):\n",
    "            for j in range(n_nodes):\n",
    "                attention_mask[node_start_idx + i, node_start_idx + j] = 1\n",
    "\n",
    "        for i in range(n_edges):\n",
    "            start_node = start_nodes[i].item() + node_start_idx\n",
    "            end_node = end_nodes[i].item() + node_start_idx\n",
    "            edge_idx = edge_start_idx + i\n",
    "\n",
    "            # Edges can attend to their corresponding nodes\n",
    "            attention_mask[edge_idx, start_node] = 1\n",
    "            attention_mask[edge_idx, end_node] = 1\n",
    "\n",
    "            # Nodes can attend to their corresponding edges\n",
    "            attention_mask[start_node, edge_idx] = 1\n",
    "            attention_mask[end_node, edge_idx] = 1\n",
    "\n",
    "    # Convert the mask to float and set masked positions to -inf and allowed positions to 0\n",
    "    attention_mask = attention_mask.float()\n",
    "    attention_mask = attention_mask.masked_fill(attention_mask == 0, float('-inf'))\n",
    "    attention_mask = attention_mask.masked_fill(attention_mask == 1, float(0.0))\n",
    "\n",
    "    # Create a vector with [n1, n2, n3, e1, e2, e3, e4]\n",
    "    orig_edge_attr_clifford = clifford_algebra.embed(edge_attr[..., None], (0,)) # now [batch * edges, 1, dim]\n",
    "    extra_edge_attr_clifford = make_edge_attr(nodes_in_clifford, edges, batch_size) # [batch*edges, #numfeatures, difference + geomprod, dim]\n",
    "    edge_attr_all = torch.cat((orig_edge_attr_clifford, extra_edge_attr_clifford), dim=1)\n",
    "    zero_padding_edges = torch.zeros(20*batch_size, 3, 8)\n",
    "    full_edge_embedding = torch.cat((zero_padding_edges, edge_attr_all), dim=1)\n",
    "\n",
    "    loc_end_clifford = clifford_algebra.embed(loc_end, (1, 2, 3))\n",
    "\n",
    "    return full_node_embedding, full_edge_embedding, loc_end_clifford, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mock_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Generate a mock batch of data with specified shapes.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int): The size of the batch to generate.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing tensors for loc_frame_0, vel_frame_0, edge_attr, charges, loc_frame_T, and edges.\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    n_nodes = 5  # Number of nodes\n",
    "    n_features = 3  # Number of spatial features (e.g., x, y, z)\n",
    "    n_edges = 20  # Number of edges\n",
    "    n_edge_features = 1  # Number of features per edge\n",
    "\n",
    "    # Generate data\n",
    "    loc_frame_0 = torch.rand(batch_size, n_nodes, n_features)\n",
    "    vel_frame_0 = torch.rand(batch_size, n_nodes, n_features)\n",
    "    edge_attr = torch.rand(batch_size, n_edges, n_edge_features)\n",
    "    charges = torch.rand(batch_size, n_nodes, 1)\n",
    "    loc_frame_T = torch.rand(batch_size, n_nodes, n_features)\n",
    "\n",
    "    # Generate edges indices\n",
    "    # For simplicity, assuming all batches share the same structure of graph\n",
    "    rows = torch.randint(0, n_nodes, (n_edges,))\n",
    "    cols = torch.randint(0, n_nodes, (n_edges,))\n",
    "    edges = torch.stack([rows, cols], dim=0).repeat(batch_size, 1, 1)  # Repeat the edge structure across the batch\n",
    "\n",
    "    return loc_frame_0, vel_frame_0, edge_attr, charges, loc_frame_T, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, batch_size):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Adding a binary feature\n",
    "        # Nodes get a [1, 0] and edges get a [0, 1]\n",
    "        node_marker = torch.zeros(5*batch_size, 1, 8)\n",
    "\n",
    "        # Create a tensor of zeros with shape [20*batch_size, 1, 8] with a 1 at the beginning (onehot)\n",
    "        edge_marker = torch.zeros(20 * batch_size, 1, 8)\n",
    "        edge_marker[:, :, 0] = 1\n",
    "\n",
    "        # Concatenate this new feature\n",
    "        self.pe = torch.cat((node_marker, edge_marker), dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape, self.pe.shape)\n",
    "        return torch.cat((x, self.pe), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionClifford(nn.Module):\n",
    "    def __init__(self, num_feat, num_nodes, num_edges, algebra):\n",
    "        super(SelfAttentionClifford, self).__init__()\n",
    "        self.num_feat = num_feat\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_edges = num_edges\n",
    "        self.algebra = algebra\n",
    "        self.q_linear = MVLinear(algebra, num_feat, 1, subspaces=True)\n",
    "        self.k_linear = MVLinear(algebra, num_feat, 1, subspaces=True)\n",
    "        self.v_linear = MVLinear(algebra, num_feat, 1, subspaces=True)\n",
    "        self.output_embedding = MVLinear(algebra, 1*2, num_feat, subspaces=True)\n",
    "        self.concat_layernorm = MVLayerNorm(algebra, 2)\n",
    "\n",
    "    def forward(self, feature_matrix):\n",
    "        bs = feature_matrix.size(0)//25\n",
    "\n",
    "        # Compute query, key, and value matrices\n",
    "        q = self.q_linear(feature_matrix)\n",
    "        k = self.k_linear(feature_matrix)\n",
    "        v = self.v_linear(feature_matrix)\n",
    "\n",
    "        # Compute dot product for attention\n",
    "        q1_reshape = q.view(25*bs, -1)\n",
    "        k1_reshape = k.view(25*bs, -1)\n",
    "\n",
    "        attn = torch.mm(q1_reshape, k1_reshape.T)  # (bs*(num_nodes + num_edges), num_feat, 8)\n",
    "        # Normalize the attention weights with d normally\n",
    "        attn = attn / math.sqrt(k.size(-1))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        v_reshaped = v.squeeze(1)\n",
    "        attention_feature_matrix = torch.matmul(attn, v_reshaped)\n",
    "        attention_feature_matrix = attention_feature_matrix.unsqueeze(1)\n",
    "\n",
    "        # Apply geometric product but might not be necessary let's check with Cong.\n",
    "        gp_feature_matrix = self.geometric_product(attention_feature_matrix, attention_feature_matrix)\n",
    "\n",
    "        concat_feature_matrix = torch.cat((attention_feature_matrix, gp_feature_matrix), dim=1)\n",
    "        normalized_concat_feature_matrix = self.concat_layernorm(concat_feature_matrix)\n",
    "        embed_output = self.output_embedding(normalized_concat_feature_matrix)\n",
    "\n",
    "        # embed_output = self.output_embedding(concat_feature_matrix)\n",
    "\n",
    "        return embed_output\n",
    "    \n",
    "    def geometric_product(self, a, b):\n",
    "        return self.algebra.geometric_product(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAST_block(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, clifford_algebra, channels):\n",
    "        super(GAST_block, self).__init__()\n",
    "        self.mvlayernorm = MVLayerNorm(clifford_algebra, channels)\n",
    "        self.self_attn = SelfAttentionClifford(7, 5, 20, clifford_algebra)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.mvlayernorm(src)\n",
    "        src = self.self_attn(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAST(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, clifford_algebra, channels):\n",
    "        super(GAST, self).__init__()\n",
    "        self.activation = MVSiLU(clifford_algebra, channels)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GAST_block(d_model, num_heads, clifford_algebra, channels) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "            src = self.activation(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBODY_Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_layers, batch_size, clifford_algebra, channels):\n",
    "        super(NBODY_Transformer, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model, batch_size)\n",
    "        self.GAST = GAST(num_layers, d_model, num_heads, clifford_algebra, channels)\n",
    "        self.MVinput = MVLinear(clifford_algebra, input_dim, d_model, subspaces=True)\n",
    "        self.MVGP = MVLinear(clifford_algebra, d_model *2, d_model, subspaces=True)\n",
    "        self.n_nodes = 5\n",
    "        self.n_edges = 20\n",
    "\n",
    "    def forward(self, nodes, edges, src_mask, batch_size):\n",
    "\n",
    "        # POSITIONAL ENCODING left out for now\n",
    "        # edges_in_clifford = self.positional_encoding(edges_in_clifford)\n",
    "\n",
    "        # Reshape nodes to [batch_size, n_nodes, n_features, feature_dim]\n",
    "        nodes = nodes.view(batch_size, self.n_nodes, nodes.size(1), nodes.size(2))\n",
    "\n",
    "        # Reshape edges to [batch_size, n_edges, n_features, feature_dim]\n",
    "        edges = edges.view(batch_size, self.n_edges, edges.size(1), edges.size(2))\n",
    "\n",
    "        combined = torch.cat((nodes, edges), dim=1) # Should be [batch_size, 25, 7, 8]\n",
    "        combined = combined.view(batch_size * (self.n_nodes + self.n_edges), combined.size(2), combined.size(3)) # Should be [batch_size*25, 7, 8]\n",
    "        src = combined\n",
    "        # src = torch.cat((nodes, edges), dim=0)\n",
    "        \n",
    "        src_MV = self.MVinput(src)\n",
    "        src_GP = clifford_algebra.geometric_product(src_MV, src_MV)\n",
    "\n",
    "        src_cat = torch.cat((src_MV, src_GP), dim=1)\n",
    "\n",
    "        src = self.MVGP(src_cat)\n",
    "\n",
    "        enc_output = self.GAST(src, src_mask)\n",
    "        output = enc_output\n",
    "        # Reshape the tensor to [batch_size, total_elements, 7, 8]\n",
    "        reshaped_output = output.view(batch_size, self.n_edges+ self.n_nodes , 7, 8)\n",
    "\n",
    "        # Extract the first 5 components (nodes) from each group of 25\n",
    "        nodes = reshaped_output[:, :self.n_nodes, :, :]\n",
    "\n",
    "        # Select the 2nd feature (index 1) of the 7 features\n",
    "        selected_feature = nodes[:, :, 1, :]\n",
    "        selected_feature = selected_feature.reshape(batch_size*self.n_nodes, 8)\n",
    "\n",
    "        # return only nodes and only the \"pos\" feature vector of the nodes\n",
    "        return selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:23,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3463870286941528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:05<00:20,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.4775753021240234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:07<00:18,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 3.6336231231689453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:10<00:15,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.566415548324585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:13<00:13,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.3960540294647217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:15<00:10,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.0230329036712646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:18<00:07,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1.5396754741668701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:20<00:05,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 1.6015619039535522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:23<00:02,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.205975890159607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:26<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.9032961130142212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 7  # feature_dim\n",
    "d_model = 7 # hidden_dim for transformer\n",
    "num_heads = 8 # number of heads in transformer\n",
    "num_layers = 6 # number of transformer layers\n",
    "batch_size = 2\n",
    "channels = 7   #????????\n",
    "num_samples = 256\n",
    "\n",
    "# Create the model\n",
    "model = NBODY_Transformer(input_dim, d_model, num_heads, num_layers, batch_size, clifford_algebra, channels)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# demo_batch = generate_mock_batch(batch_size)\n",
    "# model.train()\n",
    "# for epoch in tqdm(range(10)):\n",
    "#     batch = demo_batch\n",
    "#     optimizer.zero_grad()\n",
    "#     components = embed_nbody_graphs(batch)\n",
    "#     nodes, edges, tgt, attention_mask = components\n",
    "\n",
    "#     output = model(nodes, edges, src_mask=attention_mask, batch_size=batch_size)\n",
    "\n",
    "#     loss = criterion(output, tgt)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "nbody_data = nbody.NBody(num_samples=num_samples, batch_size=batch_size)\n",
    "train_loader = nbody_data.train_loader()\n",
    "model.train()\n",
    "for epoch in tqdm(range(10)):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        components = embed_nbody_graphs(batch)\n",
    "        nodes, edges, tgt, attention_mask = components\n",
    "\n",
    "        output = model(nodes, edges, src_mask=attention_mask, batch_size=batch_size)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
