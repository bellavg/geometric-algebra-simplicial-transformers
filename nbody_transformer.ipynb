{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DlyjQltRbzSq",
    "outputId": "87743cd2-cbee-4fdb-9fa7-8630e7fa9bf8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:53.553759Z",
     "start_time": "2024-05-15T07:55:53.551810Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('/')\n",
    "# if not os.path.exists(\"/clifford-group-equivariant-neural-networks\"):\n",
    "#     !git clone https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks.git\n",
    "# os.chdir(\"/clifford-group-equivariant-neural-networks\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('clifford-group-equivariant-neural-networks')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from algebra.cliffordalgebra import CliffordAlgebra\n",
    "from models.modules.linear import MVLinear\n",
    "from models.modules.gp import SteerableGeometricProductLayer\n",
    "from models.modules.mvlayernorm import MVLayerNorm\n",
    "from models.modules.mvsilu import MVSiLU\n",
    "from models.nbody_cggnn import CEMLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ],
   "metadata": {
    "id": "9UrS2a7Wclfm",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:54.116233Z",
     "start_time": "2024-05-15T07:55:54.113647Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the metric for 3D space (Euclidean)\n",
    "metric = [1, 1, 1]\n",
    "d = len(metric)\n",
    "\n",
    "# Initialize the Clifford Algebra for 3D\n",
    "clifford_algebra = CliffordAlgebra(metric)"
   ],
   "metadata": {
    "id": "jylAy-gjfCvi",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:54.688090Z",
     "start_time": "2024-05-15T07:55:54.682756Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def make_edge_attr(node_features, edges, batch_size):\n",
    "    edge_attributes = []\n",
    "\n",
    "    total_number_edges = edges[0].shape[0]\n",
    "\n",
    "    # Loop over all edges\n",
    "    for i in range(total_number_edges):\n",
    "\n",
    "        node1 = edges[0][i]\n",
    "        node2 = edges[1][i]\n",
    "\n",
    "        # difference between node features\n",
    "        node_i_features = node_features[node1]  # [#features(charge, loc, vel), dim]\n",
    "        node_j_features = node_features[node2]  # [#features(charge, loc, vel), dim]\n",
    "        difference = node_i_features - node_j_features\n",
    "\n",
    "        # geom product between node features\n",
    "        geom_product = clifford_algebra.geometric_product(node_i_features, node_j_features)\n",
    "\n",
    "        # now just add the 2 to get the \"representation of the edge\"\n",
    "        # Stack all\n",
    "        edge_representation = torch.cat((difference, geom_product), dim=0)\n",
    "        edge_attributes.append(edge_representation)\n",
    "\n",
    "    edge_attributes = torch.stack(edge_attributes)\n",
    "    return edge_attributes\n"
   ],
   "metadata": {
    "id": "9NcA-cRnfCyd",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:55.122445Z",
     "start_time": "2024-05-15T07:55:55.120073Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def embed_nbody_graphs(batch):\n",
    "    loc, vel, edge_attr, charges, loc_end, edges = batch\n",
    "\n",
    "    batch_size, n_nodes, _ = loc.size()\n",
    "\n",
    "    # put the mean of the pointcloud on the origin (WHY?)\n",
    "    loc_mean = loc - loc.mean(dim=1, keepdim=True)     # [batch, nodes, dim]\n",
    "\n",
    "    # ALL THESE do [batch, nodes, dim] ->>> [batch * nodes, dim]\n",
    "    loc_mean = loc_mean.float().view(-1, *loc_mean.shape[2:])\n",
    "    loc = loc.float().view(-1, *loc.shape[2:])\n",
    "    vel = vel.float().view(-1, *vel.shape[2:])\n",
    "    edge_attr = edge_attr.float().view(-1, *edge_attr.shape[2:])\n",
    "    charges = charges.float().view(-1, *charges.shape[2:])\n",
    "    loc_end = loc_end.float().view(-1, *loc_end.shape[2:])\n",
    "\n",
    "    invariants = charges\n",
    "    invariants = clifford_algebra.embed(invariants, (0,))\n",
    "    charges_in_clifford = invariants\n",
    "\n",
    "    xv = torch.stack([loc_mean, vel], dim=1) # now of the shape [batch * nodes, 2 (because its loc mean as well as vel), dim]\n",
    "    covariants = clifford_algebra.embed(xv, (1, 2, 3))\n",
    "    pos_vel_in_clifford = covariants\n",
    "\n",
    "    nodes_in_clifford = torch.cat([invariants[:, None], covariants], dim=1) # [batch * nodes, #features(charge, loc, vel), dim]\n",
    "\n",
    "    batch_index = torch.arange(batch_size, device=loc_mean.device) # torch.arange(batch_size) generates a tensor from 0 to batch_size - 1, creating a sequence that represents each graph in the batch. If batch_size is 3, this tensor will be [0, 1, 2]\n",
    "    edges = edges + n_nodes * batch_index[:, None, None] # creates separate edge number for every graph. so if edge for graph 1 is between 3 and 4, graph 2 will be between 8 and 9 (if n_nodes = 5)\n",
    "    edges = tuple(edges.transpose(0, 1).flatten(1)) # where the first element of the tuple contains all start nodes and the second contains all end nodes for edges across the entire batch. ([edges*batch], [edges*batch])\n",
    "\n",
    "    extra_edge_attr_clifford = make_edge_attr(nodes_in_clifford, edges, batch_size) # [batch*edges, #numfeatures, difference + geomprod, dim]\n",
    "\n",
    "    orig_edge_attr_clifford = clifford_algebra.embed(edge_attr[..., None], (0,)) # now [batch * edges, 1, dim]\n",
    "\n",
    "    return nodes_in_clifford, extra_edge_attr_clifford, orig_edge_attr_clifford, loc_end\n"
   ],
   "metadata": {
    "id": "mmYtdj0yfC1W",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:55.594694Z",
     "start_time": "2024-05-15T07:55:55.591247Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_mock_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Generate a mock batch of data with specified shapes.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int): The size of the batch to generate.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing tensors for loc_frame_0, vel_frame_0, edge_attr, charges, loc_frame_T, and edges.\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    n_nodes = 5  # Number of nodes\n",
    "    n_features = 3  # Number of spatial features (e.g., x, y, z)\n",
    "    n_edges = 20  # Number of edges\n",
    "    n_edge_features = 1  # Number of features per edge\n",
    "\n",
    "    # Generate data\n",
    "    loc_frame_0 = torch.rand(batch_size, n_nodes, n_features)\n",
    "    vel_frame_0 = torch.rand(batch_size, n_nodes, n_features)\n",
    "    edge_attr = torch.rand(batch_size, n_edges, n_edge_features)\n",
    "    charges = torch.rand(batch_size, n_nodes, 1)\n",
    "    loc_frame_T = torch.rand(batch_size, n_nodes, n_features)\n",
    "\n",
    "    # Generate edges indices\n",
    "    # For simplicity, assuming all batches share the same structure of graph\n",
    "    rows = torch.randint(0, n_nodes, (n_edges,))\n",
    "    cols = torch.randint(0, n_nodes, (n_edges,))\n",
    "    edges = torch.stack([rows, cols], dim=0).repeat(batch_size, 1, 1)  # Repeat the edge structure across the batch\n",
    "\n",
    "    return loc_frame_0, vel_frame_0, edge_attr, charges, loc_frame_T, edges"
   ],
   "metadata": {
    "id": "NLyPGttjfJaN",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:56.088762Z",
     "start_time": "2024-05-15T07:55:56.086196Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, batch_size):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Adding a binary feature\n",
    "        # Nodes get a [1, 0] and edges get a [0, 1]\n",
    "        # node_marker = torch.zeros(5*batch_size, 1, 8)\n",
    "        edge_marker = torch.ones(20*batch_size, 1, 8)\n",
    "\n",
    "        # Concatenate this new feature\n",
    "        # self.pe = torch.cat((node_marker, edge_marker), dim=0)\n",
    "        self.pe = edge_marker\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape, self.pe.shape)\n",
    "        return torch.cat((x, self.pe), dim=1)\n"
   ],
   "metadata": {
    "id": "Hx_h02eCcpFJ",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:56.470156Z",
     "start_time": "2024-05-15T07:55:56.467825Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn, value)\n",
    "    return output, attn\n"
   ],
   "metadata": {
    "id": "4nJIlZzccpHa",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:57.039609Z",
     "start_time": "2024-05-15T07:55:57.037422Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class GAST_block(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, clifford_algebra, channels):\n",
    "        super(GAST_block, self).__init__()\n",
    "        self.mvlayernorm = MVLayerNorm(clifford_algebra, channels)\n",
    "        self.self_attn = #TBA\n",
    "        self.mvlayernorm2 = MVLayerNorm(clifford_algebra, channels)\n",
    "        self.cemlp = CEMLP(clifford_algebra, in_features, hidden_features, out_features)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.mvlayernorm(src)\n",
    "\n",
    "\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class GAST(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads,  clifford_algebra, channels):\n",
    "        super(GAST, self).__init__()\n",
    "        self.layers = nn.ModuleList([GAST_block(d_model, num_heads) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return self.norm(src)"
   ],
   "metadata": {
    "id": "qaN6qT4Nc54n",
    "outputId": "208aef4e-476e-4caf-d021-85a2105e2b59",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:57.425166Z",
     "start_time": "2024-05-15T07:55:57.421811Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1937155272.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[11], line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    self.self_attn = #TBA\u001B[0m\n\u001B[0m                     ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class NBODY_Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_layers, mv_in_features, mv_out_features, clifford_algebra, channels):\n",
    "        super(NBODY_Transformer, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model, batch_size)\n",
    "        self.GAST = GAST(num_layers, d_model, num_heads, clifford_algebra, channels)\n",
    "\n",
    "        # subspaces is false\n",
    "        self.embedding = MVLinear(clifford_algebra, mv_out_features, mv_out_features, subspaces=False)\n",
    "        self.fc = MVLinear(clifford_algebra, d_model, input_dim, subspaces = True)\n",
    "\n",
    "        # NOG TE FIXEN\n",
    "        self.edge_to_node_shape = MVLinear(clifford_algebra, mv_in_features, mv_out_features, subspaces=True)\n",
    "\n",
    "    def forward(self, nodes_in_clifford, edges_in_clifford, src_mask, batch_size):\n",
    "\n",
    "        # POSITIONAL ENCODING NOG OVER TE SPREKEN\n",
    "        edges_in_clifford = self.positional_encoding(edges_in_clifford)\n",
    "\n",
    "        fully_embedded_edges = self.edge_to_node_shape(edges_in_clifford)\n",
    "        src = torch.cat((nodes_in_clifford, fully_embedded_edges), dim=0)\n",
    "\n",
    "        src = self.embedding(src)\n",
    "        enc_output = self.GAST(src, src_mask)\n",
    "\n",
    "        # LATER FIXEN\n",
    "        output = self.fc(enc_output)\n",
    "\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "id": "XwtwxvBjdfuW",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:55:57.921672Z",
     "start_time": "2024-05-15T07:55:57.919042Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "input_dim = 8  # feature_dim\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "mv_in_features = 8\n",
    "mv_out_features = 3\n",
    "channels = 8 # clifford algrebra vector\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = NBODY_Transformer(input_dim, d_model, num_heads, num_layers, mv_in_features, mv_out_features, clifford_algebra, channels)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 1\n",
    "batch = generate_mock_batch(batch_size)\n",
    "src_mask = None\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    batch = batch\n",
    "    components = embed_nbody_graphs(batch)\n",
    "    nodes_in_clifford, extra_edge_attr_clifford, orig_edge_attr_clifford, loc_end = components\n",
    "    edges_in_clifford = torch.cat((extra_edge_attr_clifford, orig_edge_attr_clifford), dim=1)\n",
    "    tgt = loc_end\n",
    "\n",
    "    output = model(nodes_in_clifford, edges_in_clifford, src_mask, batch_size)\n",
    "\n",
    "\n",
    "    # OUTPUT WEER TERUG NAAR TFT SHAPE [5, 3] = [nodes*batches, 3]\n",
    "\n",
    "    # originele locatie nog optellen bij prediction en dan dat vergelijken!!\n",
    "    loss = criterion(output, tgt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n"
   ],
   "metadata": {
    "id": "QxwtJgSQdfwy",
    "ExecuteTime": {
     "end_time": "2024-05-15T07:56:02.102583Z",
     "start_time": "2024-05-15T07:56:02.079222Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m channels \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m \u001B[38;5;66;03m# clifford algrebra vector\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Create the model\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m model \u001B[38;5;241m=\u001B[39m NBODY_Transformer(input_dim, d_model, num_heads, num_layers, mv_in_features, mv_out_features, clifford_algebra, channels)\n\u001B[1;32m     13\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mMSELoss()\n\u001B[1;32m     14\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n",
      "Cell \u001B[0;32mIn[12], line 4\u001B[0m, in \u001B[0;36mNBODY_Transformer.__init__\u001B[0;34m(self, input_dim, d_model, num_heads, num_layers, mv_in_features, mv_out_features, clifford_algebra, channels)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_dim, d_model, num_heads, num_layers, mv_in_features, mv_out_features, clifford_algebra, channels):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28msuper\u001B[39m(NBODY_Transformer, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_encoding \u001B[38;5;241m=\u001B[39m PositionalEncoding(d_model, batch_size)\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mGAST \u001B[38;5;241m=\u001B[39m GAST(num_layers, d_model, num_heads, clifford_algebra, channels)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# subspaces is false\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'batch_size' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
